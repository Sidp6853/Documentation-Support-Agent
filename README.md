# üìö Documentation Support Agent

A Retrieval-Augmented Generation (RAG) system for document-based Q&A.

[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/streamlit-1.28+-red.svg)](https://streamlit.io/) .

---

## üéØ Project Overview

This system enables users to upload documents (PDF, TXT, URLs, or raw text) and ask questions with guaranteed source-based answers. The agent **refuses to hallucinate** - if information isn't in the provided documents, it clearly states so.

**Built for:** Documentation Support Agent Technical Assessment

---

## üñ•Ô∏è User Interface Preview

<p align="center">
  <img src="assets/ui_screenshot.png" alt="Documentation Support Agent UI" width="800"/>
</p>

---

## ‚ú® Key Features

- ‚úÖ **Multi-source ingestion**: PDF, TXT files, web URLs, and raw text
- ‚úÖ **Semantic chunking**: Uses LangChain's SemanticChunker for intelligent text splitting
- ‚úÖ **Pure semantic search**: sentence-transformers embeddings + FAISS vector store
- ‚úÖ **Cosine similarity**: Normalized vectors for meaning-based retrieval
- ‚úÖ **Zero hallucination**: Multi-layer guardrails prevent made-up answers
- ‚úÖ **Source highlighting**: Shows exact passages with similarity scores
- ‚úÖ **Web interface**: Clean Streamlit UI with document management

---

## üèóÔ∏è Architecture

```
User Query
    ‚Üì
[Embedding Model] ‚Üí all-MiniLM-L6-v2 (384-dim vectors)
    ‚Üì
[FAISS Search] ‚Üí Cosine similarity (IndexFlatIP)
    ‚Üì
[Top-5 Chunks] ‚Üí Most relevant passages retrieved
    ‚Üì
[Gemini LLM] ‚Üí Answer generation (temp=0.1, strict prompt)
    ‚Üì
[Response] ‚Üí Answer + source citations + similarity scores
```

### Core Components

**DocumentProcessor**
- Extracts text from PDFs, TXT files, URLs
- Uses LangChain SemanticChunker for context-aware splitting
- Preserves semantic coherence across chunks

**VectorStore**
- sentence-transformers for embeddings
- FAISS IndexFlatIP for fast cosine similarity search
- Normalized vectors for semantic (not magnitude) comparison

**AnswerGenerator**
- Gemini 2.5 Flash with strict source-only prompting
- Temperature: 0.1 (low creativity = high factuality)
- Mandatory source citations in responses

**ChatBot**
- Orchestrates the full RAG pipeline
- Manages document lifecycle (ingest/clear)
- Coordinates retrieval and generation

---

## üöÄ Quick Start

### Prerequisites
- Python 3.8 or higher
- Gemini API key ([Get free key](https://ai.google.dev/))

### Installation

1. **Install dependencies**
```bash
pip install -r requirements.txt
```

2. **Run the application**
```bash
streamlit run doc_support_agent.py
```

3. **Access the interface**
- Opens automatically at `http://localhost:8501`
- Enter your Gemini API key when prompted

---

## üìñ Usage

### Step 1: Initialize
Enter your Gemini API key in the text field. Wait for "‚úÖ Chatbot initialized successfully!"

### Step 2: Upload Documents
Choose from three options:
- **Upload PDF/TXT**: Select local files
- **Enter URL**: Paste webpage URLs for scraping
- **Paste Text**: Directly input text content

Multiple documents can be added sequentially.

### Step 3: Ask Questions
Type your question in the text field. The system will:
1. Search for relevant chunks (semantic search)
2. Generate answer using only those sources
3. Display answer with source citations
4. Show source excerpts with similarity scores

### Step 4: Clear Documents (Optional)
Click "üóëÔ∏è Clear All Documents" to remove all ingested data and start fresh.

---

## üõ°Ô∏è Hallucination Prevention Strategy

### Three-Layer Defense

**Layer 1: Strict Prompting**
```
"Answer ONLY using information from the sources below"
"DO NOT use any external knowledge"
"If sources don't contain enough info, say so clearly"
```

**Layer 2: Low Temperature (0.1)**
- Minimizes LLM creativity and randomness
- Ensures deterministic, grounded responses
- Reduces likelihood of invented information

**Layer 3: Mandatory Citations**
- LLM must reference [Source 1], [Source 2], etc.
- Makes grounding transparent and verifiable
- Easy to trace answers back to documents

**Layer 4: Semantic Filtering**
- Only retrieves chunks above relevance threshold
- Top-k retrieval (default: 5 chunks)
- Prevents irrelevant context from confusing LLM

---

## üî¨ Technical Deep Dive

### Why Semantic Chunking?

Traditional fixed-size chunking (e.g., 1000 characters) often breaks mid-sentence or mid-thought. **LangChain's SemanticChunker** splits text based on semantic coherence:

```python
# Traditional chunking problems:
"...Python supports OOP. |CHUNK BREAK| Python has simple syntax..."
# Context lost! Each chunk lacks full meaning.

# Semantic chunking preserves context:
"...Python supports OOP. Python has simple syntax..." 
# Complete thoughts stay together.
```

### Why Normalize Vectors?

```python
# Without normalization (Euclidean distance)
v1 = [0.5, 0.5]   # Short vector
v2 = [5.0, 5.0]   # Long vector, SAME direction
distance = 6.36   # Seems very different!

# With normalization (Cosine similarity)
faiss.normalize_L2(embeddings)
v1_norm = [0.707, 0.707]
v2_norm = [0.707, 0.707]
similarity = 1.0  # Correctly identifies as similar!
```

**Key insight:** For text, we care about semantic **direction** (meaning), not vector **magnitude** (arbitrary scale). Normalization + IndexFlatIP gives us pure cosine similarity.

### Retrieval Pipeline

1. **Query encoding**: Convert question to 384-dim embedding
2. **Normalization**: L2-normalize query vector
3. **FAISS search**: IndexFlatIP computes dot products (= cosine similarity for normalized vectors)
4. **Top-k selection**: Return 5 most similar chunks
5. **Context building**: Combine chunks for LLM

---
## üîß Configuration Options

### Chunking (in DocumentProcessor)
```python
# Automatic semantic-based chunking
# No manual chunk_size or overlap needed
# LangChain determines optimal boundaries
```

### Retrieval (in VectorStore.search)
```python
k = 5  # Number of chunks to retrieve
# Adjustable: chatbot.query(question, k=10)
```

### LLM Generation (in AnswerGenerator)
```python
generation_config = {
    "temperature": 0.1,        # Low = factual, high = creative
    "top_p": 0.9,             # Nucleus sampling
    "max_output_tokens": 1500  # Response length limit
}
```

---

## üéì Technical Decisions & Trade-offs

### Why sentence-transformers/all-MiniLM-L6-v2?
- **Speed**: Fast inference, only 384 dimensions
- **Quality**: Good semantic understanding for general text
- **Size**: 80MB model (reasonable download)

**Alternative:** all-mpnet-base-v2 (768-dim, better quality, slower)

### Why FAISS?
- **Performance**: Millisecond search even with 100k+ vectors
- **Memory efficient**: Optimized C++ implementation
- **Scalable**: Supports billions of vectors
- **Industry standard**: Developed by Meta AI

**Alternative:** Pinecone, Weaviate (managed services, more features)

### Why Gemini 2.5 Flash?
- **Speed**: Fast response times 
- **Quality**: Good instruction following
- **Cost**: Free tier available
- **Reliability**: Handles strict prompting well

**Alternative:** GPT-4, Claude (better quality, higher cost) or Transformer - based Open Source Models such as LiquidAI/LFM2-1.2B-RAG

### Why LangChain SemanticChunker?
- **Context preservation**: Doesn't split mid-thought
- **Semantic coherence**: Uses embeddings to find boundaries
- **Better retrieval**: More meaningful chunks = better matches

**Alternative:** Fixed-size chunking (simpler, less accurate)

---

## üì¶ Project Structure

```
.
‚îú‚îÄ‚îÄ doc_support_agent.py                 # Main Streamlit application
‚îú‚îÄ‚îÄ requirements.txt       # Python dependencies
‚îú‚îÄ‚îÄ README.md             # This file
‚îî‚îÄ‚îÄ .gitignore            # Git exclusions (API keys, etc.)
```

### Class Hierarchy

```
ChatBot
  ‚îú‚îÄ‚îÄ DocumentProcessor (ingestion + chunking)
  ‚îú‚îÄ‚îÄ VectorStore (embeddings + search)
  ‚îú‚îÄ‚îÄ AnswerGenerator (LLM interface)
  ‚îî‚îÄ‚îÄ Similar_source (formatting utilities)
```

---

## üö® Known Limitations

1. **In-memory storage**: Documents cleared on app restart
   - *Production fix*: Use persistent vector DB (Pinecone, Weaviate)

2. **No conversation history**: Each query is independent
   - *Production fix*: Implement chat memory with context window

3. **English-optimized**: Model trained primarily on English
   - *Production fix*: Use multilingual models (paraphrase-multilingual)

4. **PDF quality dependent**: Scanned PDFs won't extract text
   - *Production fix*: Add OCR (pytesseract, AWS Textract)

5. **Single-session**: No user accounts or saved documents
   - *Production fix*: Add authentication and database storage

---

## üîÆ Future Enhancements

- [ ] **Re-ranking stage**: Add cross-encoder for better precision
- [ ] **Conversation memory**: Track dialogue context
- [ ] **Document versioning**: Update docs without full re-index
- [ ] **Batch upload**: Process multiple files simultaneously
- [ ] **Query caching**: Store common question-answer pairs
- [ ] **Advanced filters**: Filter by document source, date, etc.
- [ ] **Export functionality**: Save Q&A pairs as markdown/PDF
- [ ] **Analytics**: Track popular queries, retrieval quality

---

## üß™ Testing Recommendations

### Test Cases

1. **Clear Answer Test**
   - Upload Python tutorial
   - Ask: "What is a list comprehension?"
   - ‚úÖ Should get detailed answer with sources

2. **Hallucination Prevention Test**
   - Same document
   - Ask: "How do I use React hooks?"
   - ‚úÖ Should refuse (not in Python docs)

3. **Multi-source Test**
   - Upload multiple documents
   - Ask question spanning both
   - ‚úÖ Should synthesize from multiple sources

4. **Edge Cases**
   - Empty query ‚Üí validation error
   - No documents uploaded ‚Üí warning message
   - Malformed PDF ‚Üí graceful error handling

---

### ‚úÖ File/URL/Text Ingestion
- [x] PDF files (PyPDF2)
- [x] TXT files (native Python)
- [x] URLs (BeautifulSoup + requests)
- [x] Raw text (direct input)
- [x] Intelligent chunking (SemanticChunker)

### ‚úÖ Embedding and Retrieval
- [x] HuggingFace model (sentence-transformers)
- [x] Vector database (FAISS in-memory)
- [x] Semantic search (cosine similarity)
- [x] **No keyword matching** (pure embeddings)

### ‚úÖ Chatbot Interface
- [x] Question input
- [x] Strictly source-based answers
- [x] Source passage highlighting
- [x] Similarity scores displayed
- [x] Clean web UI (Streamlit)

### ‚úÖ Hallucination Guardrails
- [x] Strict prompting
- [x] Low temperature (0.1)
- [x] Mandatory citations
- [x] Clear "insufficient information" responses
- [x] No invented content

### ‚úÖ Code Quality
- [x] Modular structure (4 main classes)
- [x] Clear separation of concerns
- [x] Type hints (Pydantic models)
- [x] Error handling
- [x] Clean, readable code

---


## üèÜ What Makes This Solution Stand Out

1. **Modern RAG**: Uses current best practices (semantic chunking, normalized vectors)
2. **Zero to minimal hallucination**: Multiple layers of prevention
3. **Clean code**: Well-structured, typed, documented

---

## üìû Support

For setup issues:
1. Check `requirements.txt` - all dependencies installed?
2. Python 3.8+ installed? Check with `python --version`
3. Valid Gemini API key from https://ai.google.dev/
4. First run downloads model (~80MB) - wait for completion

---

## üë§ Author

Documentation Support Agent - Siddhi Pandya

---

**Built with ‚ù§Ô∏è for accurate, trustworthy document Q&A**